{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os alunos deverão avaliar o desempenho dos algoritmos segundo três aspectos: \n",
    "#tempo, espaço, e qualidade da solução. Cada algoritmo deverá ser executado com cada \n",
    "#instância do conjunto de teste, e as variáveis de desempenho devem ser coletadas para cada execução.\n",
    "#O tempo de processamento deve ser limitado a 30min. Após esse prazo a execução do algoritmo deve ser abortada e os\n",
    "#dados referentes à execução colocados como NA (não-disponível)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install memory-profiler\n",
    "#!pip install timeout_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import networkx as nx\n",
    "from timeout_decorator import timeout\n",
    "from queue import PriorityQueue\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from memory_profiler import profile, memory_usage\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME_SEG = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando dataset com o nome dos arquivos e ordenando em ordem crescente de acordo com o tamanho dos nós"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dataset    Nós               Limiar\n",
      "0      eil51     51                  426\n",
      "1   berlin52     52                 7542\n",
      "2       st70     70                  675\n",
      "3      eil76     76                  538\n",
      "4       pr76     76               108159\n",
      "..       ...    ...                  ...\n",
      "73   rl11849  11849      [920847,923368]\n",
      "74  usa13509  13509  [19947008,19982889]\n",
      "75  brd14051  14051      [468942,469445]\n",
      "76    d15112  15112    [1564590,1573152]\n",
      "77    d18512  18512      [644650,645488]\n",
      "\n",
      "[78 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = 'datasets_limiar.txt'\n",
    "\n",
    "df_dados_dataset = pd.read_csv(caminho_arquivo, delimiter='\\t')\n",
    "\n",
    "# Ordenando pelo tamanho dos nós\n",
    "df = df.sort_values(by='Nós')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distancia_euclidiana(pontos):\n",
    "    np.array(pontos)\n",
    "    arestas_euclidiana = []\n",
    "    tamanho_pontos = len(pontos)\n",
    "\n",
    "    for i in range(tamanho_pontos):\n",
    "        for j in range(i+1, tamanho_pontos):\n",
    "            # Distância Euclidiana\n",
    "            ponto_i = np.array(pontos[i])\n",
    "            ponto_j = np.array(pontos[j])\n",
    "            distancia_euclidiana = np.linalg.norm(ponto_i - ponto_j)\n",
    "            arestas_euclidiana.append((i, j, distancia_euclidiana))\n",
    "\n",
    "    return arestas_euclidiana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pontos 0 e 1: Distância Euclidiana = 1.4142135623730951\n",
      "Pontos 0 e 2: Distância Euclidiana = 2.8284271247461903\n",
      "Pontos 0 e 3: Distância Euclidiana = 4.242640687119285\n",
      "Pontos 1 e 2: Distância Euclidiana = 1.4142135623730951\n",
      "Pontos 1 e 3: Distância Euclidiana = 2.8284271247461903\n",
      "Pontos 2 e 3: Distância Euclidiana = 1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "pontos = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n",
    "arestas_euclidiana = distancia_euclidiana(pontos)\n",
    "\n",
    "for aresta in arestas_euclidiana:\n",
    "    print(f'Pontos {aresta[0]} e {aresta[1]}: Distância Euclidiana = {aresta[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando função extrair informações dos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(idx_dataset, df_dados_dataset):\n",
    "    name_dataset = df_dados_dataset.loc[idx_dataset, 'Dataset']\n",
    "    tsp_file_path = \"datasets/\" + name_dataset + \".tsp\"\n",
    "    # Lendo o arquivo\n",
    "    with open(tsp_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Encontrando a seção NODE_COORD_SECTION\n",
    "    coord_section_index = lines.index('NODE_COORD_SECTION\\n')\n",
    "\n",
    "    # Criando um grafo direcionado\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Adicionando nós e coordenadas ao grafo\n",
    "    for line in lines[coord_section_index + 1:]:\n",
    "        # Ignorar a linha EOF\n",
    "        if line.strip().lower() == 'eof':\n",
    "            break\n",
    "\n",
    "        node_info = line.split()\n",
    "\n",
    "        if len(node_info) >= 3:\n",
    "            node_id = int(node_info[0])\n",
    "            x_coord = float(node_info[1])\n",
    "            y_coord = float(node_info[2])\n",
    "            G.add_node(node_id, pos=(x_coord, y_coord))\n",
    "    \n",
    "    # Extrair apenas as coordenadas\n",
    "    cordenadas = [pos for _, pos in G.nodes(data='pos')]\n",
    "            \n",
    "    numero_de_nos = int(df_dados_dataset.loc[idx_dataset, 'Nós'])\n",
    "    limiar = int(df_dados_dataset.loc[idx_dataset, 'Limiar'])\n",
    "    \n",
    "    return cordenadas, numero_de_nos, limiar, name_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero_de_nos:  51\n",
      "Limiar:  426\n",
      "Cordenadas:\n",
      "[(37.0, 52.0), (49.0, 49.0), (52.0, 64.0), (20.0, 26.0), (40.0, 30.0), (21.0, 47.0), (17.0, 63.0), (31.0, 62.0), (52.0, 33.0), (51.0, 21.0), (42.0, 41.0), (31.0, 32.0), (5.0, 25.0), (12.0, 42.0), (36.0, 16.0), (52.0, 41.0), (27.0, 23.0), (17.0, 33.0), (13.0, 13.0), (57.0, 58.0), (62.0, 42.0), (42.0, 57.0), (16.0, 57.0), (8.0, 52.0), (7.0, 38.0), (27.0, 68.0), (30.0, 48.0), (43.0, 67.0), (58.0, 48.0), (58.0, 27.0), (37.0, 69.0), (38.0, 46.0), (46.0, 10.0), (61.0, 33.0), (62.0, 63.0), (63.0, 69.0), (32.0, 22.0), (45.0, 35.0), (59.0, 15.0), (5.0, 6.0), (10.0, 17.0), (21.0, 10.0), (5.0, 64.0), (30.0, 15.0), (39.0, 10.0), (32.0, 39.0), (25.0, 32.0), (25.0, 55.0), (48.0, 28.0), (56.0, 37.0), (30.0, 40.0)]\n"
     ]
    }
   ],
   "source": [
    "idx_dataset = 0\n",
    "cordenadas, numero_de_nos, limiar,  name_dataset = dataset_info(idx_dataset, df_dados_dataset)\n",
    "\n",
    "print(\"Numero_de_nos: \", numero_de_nos)\n",
    "print(\"Limiar: \", limiar)\n",
    "\n",
    "print(\"Cordenadas:\")\n",
    "print(cordenadas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando funções para gerar datasets da análise dos algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando algoritmos para solucionar o problema do caxeiro viajante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Branch and bound (Solução ótima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, bound, nivel, custo, solucao):\n",
    "        # 'bound': Limite associado ao nó na árvore do Branch and Bound.\n",
    "        self.bound = bound\n",
    "        # 'nivel': Nível na árvore em que o nó está localizado.\n",
    "        self.nivel = nivel\n",
    "        # 'custo': Custo associado ao nó (custo acumulado até o nó atual).\n",
    "        self.custo = custo\n",
    "        # 'solucao': Solução parcial associada ao nó.\n",
    "        self.solucao = solucao\n",
    "\n",
    "    # Método para comparar dois nós com base em seus limites (bound).\n",
    "    # Esse método é utilizado para determinar a ordem de prioridade em uma fila de prioridade.\n",
    "    def __lt__(self, outro):\n",
    "        return self.bound < outro.bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound(G, nodes):\n",
    "    # Inicializando a estimativa total\n",
    "    estimativa = 0\n",
    "    \n",
    "    size_nodes = len(nodes)\n",
    "    # Convertendo a lista de vértices em listas de arestas\n",
    "    edges = [(nodes[i], nodes[i+1]) for i in range(size_nodes-1)]\n",
    "    \n",
    "    # Calculando a estimativa com base nas arestas\n",
    "    for (u, v) in edges:\n",
    "        # Somando o dobro do peso da aresta ao custo total\n",
    "        estimativa += (2 * G[u][v]['weight'])\n",
    "    \n",
    "   \n",
    "    # Iterando sobre os vértices do grafo\n",
    "    for u in G.nodes():          \n",
    "        # Filtrando as arestas que incidem no vértice 'u'\n",
    "        arestas_u = list(filter(lambda x: x[0] == u or x[1] == u, edges))\n",
    "        \n",
    "        size_arestas_u = len(arestas_u)\n",
    "        # Verificando se 'u' tem menos de duas arestas incidentes\n",
    "        if size_arestas_u < 2:\n",
    "            # Obtendo os dados das arestas incidentes a 'u' e ordenando por peso\n",
    "            dados_u = list(G.edges(u, data=True))\n",
    "            dados_u = sorted(dados_u, key=lambda x: x[2]['weight'])\n",
    "            w1, w2 = dados_u[0][2]['weight'], dados_u[1][2]['weight']\n",
    "            \n",
    "            # Caso 'u' não tenha arestas incidentes, adicionamos os dois menores pesos\n",
    "            if size_arestas_u == 0:\n",
    "                estimativa += w1\n",
    "                estimativa += w2\n",
    "            \n",
    "            # Caso 'u' tenha uma aresta incidente, adicionamos o menor peso necessário\n",
    "            elif size_arestas_u == 1:\n",
    "                u, v = arestas_u[0]\n",
    "                peso_necessario = G[u][v]['weight']\n",
    "                estimativa += w1 if w1 < peso_necessario else w2\n",
    "    \n",
    "    # Arredondando para cima e retornando a estimativa dividida por 2\n",
    "    output = ceil(estimativa / 2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch_and_bound(grafo):\n",
    "    # Copiando o grafo para evitar modificações indesejadas\n",
    "    G = grafo.copy()\n",
    "\n",
    "    # Criando o nó raiz da árvore do Branch and Bound\n",
    "    raiz = Node(bound(G, []), 1, 0, [0])\n",
    "    \n",
    "    # Inicializando a fila de prioridade com o nó raiz\n",
    "    fila_prioridade = PriorityQueue()\n",
    "    fila_prioridade.put(raiz)\n",
    "\n",
    "    # Inicializando variáveis para armazenar a melhor solução encontrada\n",
    "    melhor_solucao = np.inf\n",
    "    solucao_encontrada = []\n",
    "    \n",
    "    # Percorrendo a fila de prioridade\n",
    "    while not fila_prioridade.empty():\n",
    "\n",
    "        # Obtendo o próximo nó da fila de prioridade\n",
    "        no = fila_prioridade.get()\n",
    "\n",
    "        # Verificando se o nó atual é uma folha da árvore\n",
    "        if no.nivel == G.number_of_nodes():\n",
    "            # Calculando o custo total da solução encontrada\n",
    "            custo_total = no.custo + G[no.solucao[-1]][0]['weight']\n",
    "            \n",
    "            # Atualizando a melhor solução, se aplicável\n",
    "            if custo_total < melhor_solucao:\n",
    "                melhor_solucao = custo_total\n",
    "                solucao_encontrada = no.solucao + [0]\n",
    "        elif no.bound < melhor_solucao and no.nivel < G.number_of_nodes():\n",
    "            v = no.solucao[-1]\n",
    "            # Explorando os nós filhos do nó atual\n",
    "            for k in range(G.number_of_nodes()):\n",
    "                if k not in no.solucao and G.has_edge(v, k):\n",
    "                    nova_solucao = no.solucao + [k]\n",
    "                    estimativa = bound(G, nova_solucao)\n",
    "                    # Verificando se a estimativa é melhor do que a melhor solução conhecida\n",
    "                    if estimativa < melhor_solucao:\n",
    "                        novo_custo = no.custo + G[v][k]['weight']\n",
    "                        # Adicionando o novo nó à fila de prioridade\n",
    "                        fila_prioridade.put(Node(estimativa, no.nivel+1, novo_custo, nova_solucao))\n",
    "\n",
    "    # Retornando a melhor solução encontrada, o custo, o tempo de execução e se o tempo limite foi excedido\n",
    "    return solucao_encontrada, melhor_solucao\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx_dataset = 0\n",
    "#cordenadas, numero_de_nos, limiar = dataset_info(idx_dataset, df_dados_dataset)\n",
    "\n",
    "#arestas_euclidiana = distancia_euclidiana(cordenadas)\n",
    "#G = nx.Graph()\n",
    "#G.add_weighted_edges_from(arestas_euclidiana)\n",
    "#_, comprimento = branch_and_bound(G)\n",
    "\n",
    "#print(\"Valor esperado: \", limiar )\n",
    "#print(\"Valor obtido: \",comprimento )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Christofides (1.5 aproximado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(MAX_TIME_SEG) \n",
    "def christofides(grafo):\n",
    "    G = grafo.copy()\n",
    "\n",
    "    # Encontrando a árvore geradora mínima do grafo\n",
    "    AGM = nx.minimum_spanning_tree(G)\n",
    "\n",
    "    # Criando o conjunto de vértices com grau ímpar e construindo um subgrafo induzido\n",
    "    vertices_grau_impar = [vertice for vertice in AGM.nodes() if AGM.degree(vertice) % 2 == 1]\n",
    "    grafo_induzido = G.subgraph(vertices_grau_impar)\n",
    "\n",
    "    # Invertendo o peso das arestas para calcular o emparelhamento perfeito de peso mínimo\n",
    "    for u, v in grafo_induzido.edges():\n",
    "        grafo_induzido[u][v]['weight'] *= -1\n",
    "\n",
    "    # Encontrando o emparelhamento perfeito de peso mínimo e revertendo os pesos\n",
    "    emparelhamento_min_peso = nx.max_weight_matching(grafo_induzido, maxcardinality=True)\n",
    "\n",
    "    # Criando um subgrafo induzido com as arestas do emparelhamento perfeito de peso mínimo\n",
    "    grafo_emparelhamento_min_peso = G.edge_subgraph(emparelhamento_min_peso)\n",
    "\n",
    "    # Criando um multigrafo com os vértices de G e as arestas da AGM e do emparelhamento perfeito de peso mínimo\n",
    "    multigrafo = nx.MultiGraph()\n",
    "    multigrafo.add_weighted_edges_from(AGM.edges.data('weight'))\n",
    "    multigrafo.add_weighted_edges_from(grafo_emparelhamento_min_peso.edges.data('weight'))\n",
    "\n",
    "    # Computando o circuito euleriano\n",
    "    circuito_euleriano = [u for u, v in nx.eulerian_circuit(multigrafo, source=0)]\n",
    "\n",
    "    # Removendo vértices duplicados para construir um circuito hamiltoniano\n",
    "    caminho = list(dict.fromkeys(circuito_euleriano))\n",
    "    caminho.append(0)\n",
    "\n",
    "    # Calculando o comprimento do caminho encontrado\n",
    "    comprimento = sum(grafo[u][v]['weight'] for u, v in zip(caminho, caminho[1:]))\n",
    "\n",
    "    return caminho, comprimento, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### twice around the tree (2 aproximado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twice_around_the_tree(grafo):\n",
    "    # Copia o grafo para evitar modificações indesejadas\n",
    "    G = grafo.copy()\n",
    "\n",
    "    # Encontra a Árvore Geradora Mínima do grafo\n",
    "    arvore_geradora_minima = nx.minimum_spanning_tree(G)\n",
    "\n",
    "    # Realiza uma caminhada em pré-ordem pela árvore, usando o vértice 0 como raiz\n",
    "    caminho = list(nx.dfs_preorder_nodes(arvore_geradora_minima, source=0))\n",
    "    caminho.append(0)\n",
    "\n",
    "    # Calcula o comprimento do caminho encontrado\n",
    "    comprimento = sum(G[u][v]['weight'] for u, v in zip(caminho, caminho[1:]))\n",
    "\n",
    "    # Retorna a ordem dos vértices do caminho, seu tamanho e o tempo de execução\n",
    "    return caminho, comprimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo analise de cada algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analise_dados(idx_dataset, df_dados_dataset, dataset_info, alg):\n",
    "    cordenadas, numero_de_nos, limiar, name_dataset = dataset_info(idx_dataset, df_dados_dataset)\n",
    "    aprox = 0\n",
    "    tempo_execucao = 0\n",
    "\n",
    "    arestas_euclidiana = distancia_euclidiana(cordenadas)\n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(arestas_euclidiana)\n",
    "    \n",
    "    if alg == \"bb\":\n",
    "        start_time = time.time()\n",
    "        _, comprimento = branch_and_bound(G)\n",
    "        end_time = time.time()\n",
    "        tempo_execucao = end_time - start_time\n",
    "        apro = 1\n",
    "       \n",
    "    elif alg == \"chr\":\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            _, comprimento = christofides(G)\n",
    "            end_time = time.time()\n",
    "            tempo_execucao = end_time - start_time\n",
    "            apro = 1.5\n",
    "        except TimeoutError:\n",
    "            print(\"A execução foi interrompida devido ao tempo limite.\")\n",
    "\n",
    "        \n",
    "    elif alg == \"tat\":\n",
    "        start_time = time.time()\n",
    "        _, comprimento = twice_around_the_tree(G)\n",
    "        end_time = time.time()\n",
    "        tempo_execucao = end_time - start_time\n",
    "        apro = 2\n",
    "\n",
    "    taxa_de_aproximacao = (comprimento / limiar)\n",
    "    valor_max_esperado = apro * limiar\n",
    "\n",
    "    return limiar, valor_max_esperado, comprimento, taxa_de_aproximacao, numero_de_nos, name_dataset, tempo_execucao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'signal' has no attribute 'SIGALRM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-580-195c251bc8a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0midx_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0malg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"chr\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlimiar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalor_max_esperado\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomprimento\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtaxa_de_aproximação\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumero_de_nos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtempo_execucao\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalise_dados\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_dados_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nome dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Limiar: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimiar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-579-4e3ea78bc6ed>\u001b[0m in \u001b[0;36manalise_dados\u001b[1;34m(idx_dataset, df_dados_dataset, dataset_info, alg)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomprimento\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchristofides\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mtempo_execucao\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\timeout_decorator\\timeout_decorator.py\u001b[0m in \u001b[0;36mnew_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mnew_seconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnew_seconds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                     \u001b[0mold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIGALRM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m                     \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetitimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITIMER_REAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_seconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'signal' has no attribute 'SIGALRM'"
     ]
    }
   ],
   "source": [
    "idx_dataset = 1\n",
    "alg = \"chr\"\n",
    "limiar, valor_max_esperado, comprimento, taxa_de_aproximação, numero_de_nos, name_dataset, tempo_execucao = analise_dados(idx_dataset, df_dados_dataset, dataset_info, alg)\n",
    "print(\"nome dataset\", name_dataset)\n",
    "print(\"Limiar: \", limiar)\n",
    "print(f'Valor esperado: menor que {valor_max_esperado}')\n",
    "print(\"Valor obtido: \",comprimento )\n",
    "print(\"Taxa de aproximação: \", (comprimento/limiar))\n",
    "print(\"número de nos, :\", numero_de_nos)\n",
    "print(\"tempo execução\", tempo_execucao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
